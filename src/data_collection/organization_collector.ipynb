{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import geopy\n",
    "import os\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set higher logging level for the noisy loggers\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_MODEL = os.getenv(\"GROQ_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_FETCH_ORGANIZATION_INFO = 50\n",
    "BATCH_SIZE_FETCH_WIKI_URL = 100\n",
    "BATCH_SIZE_FETCH_WIKI_PAGE_HTML = 20\n",
    "TIME_OUT = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_batch_async(fetch_function, data: list) -> list[dict]:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_function(session, item) for item in data]\n",
    "        res = await asyncio.gather(*tasks)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/models_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_names = df[\"model_name\"].apply(lambda x: x.split(\"/\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_names = author_names.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_organization_info(session, organization_name: str) -> dict:\n",
    "    url = f\"https://huggingface.co/api/organizations/{organization_name}/overview\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    "    async with session.get(url, headers=headers) as response:\n",
    "        # Add timeout if status code is 429 (Too Many Requests)\n",
    "        if response.status == 429:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", TIME_OUT))\n",
    "            await asyncio.sleep(retry_after)\n",
    "            # Retry the request after waiting\n",
    "            async with session.get(url, headers=headers) as retry_response:\n",
    "                return await retry_response.json()\n",
    "        return await response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [03:34<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "organization_info = []\n",
    "\n",
    "# Calculate the number of complete batches\n",
    "num_complete_batches = len(author_names) // BATCH_SIZE_FETCH_ORGANIZATION_INFO\n",
    "# Check if there's a remainder that needs an additional batch\n",
    "has_remainder = len(author_names) % BATCH_SIZE_FETCH_ORGANIZATION_INFO > 0\n",
    "# Total number of batches needed\n",
    "total_batches = num_complete_batches + (1 if has_remainder else 0)\n",
    "\n",
    "for i in tqdm(range(total_batches)):\n",
    "    start_idx = i * BATCH_SIZE_FETCH_ORGANIZATION_INFO\n",
    "    end_idx = min((i + 1) * BATCH_SIZE_FETCH_ORGANIZATION_INFO, len(author_names))\n",
    "\n",
    "    res = await fetch_batch_async(\n",
    "        fetch_organization_info,\n",
    "        author_names[start_idx:end_idx],\n",
    "    )\n",
    "    organization_info.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info = [info for info in organization_info if \"error\" not in info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info = pd.DataFrame(organization_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_wikipedia_url(session, organization_name: str) -> str:\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch={organization_name}&format=json\"\n",
    "    async with session.get(url) as search_response:\n",
    "        search_data = await search_response.json()\n",
    "        if search_data.get(\"query\", {}).get(\"search\", []):\n",
    "            page_id = search_data[\"query\"][\"search\"][0][\"pageid\"]\n",
    "            return f\"https://en.wikipedia.org/?curid={page_id}\"\n",
    "    return \"\"  # No results found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:27<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "wiki_page_urls = []\n",
    "\n",
    "# Calculate the number of complete batches\n",
    "num_complete_batches = len(organization_info) // BATCH_SIZE_FETCH_WIKI_URL\n",
    "# Check if there's a remainder that needs an additional batch\n",
    "has_remainder = len(organization_info) % BATCH_SIZE_FETCH_WIKI_URL > 0\n",
    "# Total number of batches needed\n",
    "total_batches = num_complete_batches + (1 if has_remainder else 0)\n",
    "\n",
    "for i in tqdm(range(total_batches)):\n",
    "    start_idx = i * BATCH_SIZE_FETCH_WIKI_URL\n",
    "    end_idx = min((i + 1) * BATCH_SIZE_FETCH_WIKI_URL, len(organization_info))\n",
    "\n",
    "    res = await fetch_batch_async(\n",
    "        get_wikipedia_url,\n",
    "        organization_info[\"name\"][start_idx:end_idx],\n",
    "    )\n",
    "    wiki_page_urls.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info[\"wikipedia_url\"] = wiki_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info = organization_info[organization_info[\"wikipedia_url\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headquarters_from_wikipage(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    headquarters = None\n",
    "    infobox = soup.select_one(\"table.infobox\")\n",
    "    if infobox:\n",
    "        for row in infobox.find_all(\"tr\"):\n",
    "            header = row.find(\"th\")\n",
    "            if header and (\"Headquarters\" in header.text or \"Location\" in header.text):\n",
    "                td = row.find(\"td\")\n",
    "                if td:\n",
    "                    headquarters = td.get_text()\n",
    "                    if not headquarters:\n",
    "                        headquarters = \"\".join([text for text in td.stripped_strings])\n",
    "                    if headquarters:\n",
    "                        headquarters = re.sub(r\"\\s+\", \" \", headquarters).strip()\n",
    "                break\n",
    "        return headquarters\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_wikipedia_page_html(session, url: str) -> str:\n",
    "    async with session.get(url) as search_response:\n",
    "        return await search_response.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [01:33<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "wiki_page_htmls = []\n",
    "\n",
    "# Calculate the number of complete batches\n",
    "num_complete_batches = len(organization_info) // BATCH_SIZE_FETCH_WIKI_PAGE_HTML\n",
    "# Check if there's a remainder that needs an additional batch\n",
    "has_remainder = len(organization_info) % BATCH_SIZE_FETCH_WIKI_PAGE_HTML > 0\n",
    "\n",
    "total_batches = num_complete_batches + (1 if has_remainder else 0)\n",
    "\n",
    "for i in tqdm(range(total_batches)):\n",
    "    start_idx = i * BATCH_SIZE_FETCH_WIKI_PAGE_HTML\n",
    "    end_idx = min((i + 1) * BATCH_SIZE_FETCH_WIKI_PAGE_HTML, len(organization_info))\n",
    "\n",
    "    res = await fetch_batch_async(\n",
    "        get_wikipedia_page_html,\n",
    "        organization_info[\"wikipedia_url\"][start_idx:end_idx],\n",
    "    )\n",
    "    wiki_page_htmls.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1142/1142 [01:15<00:00, 15.09it/s]\n"
     ]
    }
   ],
   "source": [
    "headquarters = []\n",
    "for html in tqdm(wiki_page_htmls):\n",
    "    headquarters.append(extract_headquarters_from_wikipage(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info[\"headquarters\"] = headquarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info.dropna(subset=[\"headquarters\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info = organization_info[\n",
    "    (organization_info[\"numUsers\"] > 2)\n",
    "    & (organization_info[\"numModels\"] > 1)\n",
    "    & (organization_info[\"numFollowers\"] > 200)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"name\", \"numUsers\", \"numModels\", \"numFollowers\", \"headquarters\"]\n",
    "organization_info = organization_info[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Location(BaseModel):\n",
    "    country: str\n",
    "    city: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = ChatGroq(api_key=GROQ_API_KEY, model_name=GROQ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = groq_client.with_structured_output(Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the country and city (if presented) from the following address:\n",
    "{address}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [02:23<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "locations = []\n",
    "for address in tqdm(organization_info[\"headquarters\"]):\n",
    "    locations.append(groq_client.invoke(prompt.format(address=address)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info[\"country\"] = [location.country for location in locations]\n",
    "organization_info[\"city\"] = [location.city for location in locations]\n",
    "organization_info.drop(columns=[\"headquarters\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info = pd.read_csv(\"../../data/organization_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocoder = geopy.geocoders.Nominatim(user_agent=\"DWV project\")\n",
    "\n",
    "for i in range(len(organization_info)):\n",
    "    sleep(1)\n",
    "    if organization_info.loc[i, \"city\"] is not np.nan:\n",
    "        coords = geocoder.geocode(organization_info.loc[i, \"city\"])[1]\n",
    "        organization_info.loc[i, \"latitude\"] = coords[0]\n",
    "        organization_info.loc[i, \"longitude\"] = coords[1]\n",
    "    else:\n",
    "        coords = geocoder.geocode(organization_info.loc[i, \"country\"])[1]\n",
    "        organization_info.loc[i, \"latitude\"] = coords[0]\n",
    "        organization_info.loc[i, \"longitude\"] = coords[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "organization_info.to_csv(\"../../data/organization_info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
